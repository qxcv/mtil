{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collating results from `comparison_runs.py`\n",
    "\n",
    "Going to use this for my paper. Might copy some portions from `collate_csvs.py`, which this notebook supersedes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import colorsys\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('.')\n",
    "from comparison_runs import EnvName\n",
    "del sys.path[-1]\n",
    "\n",
    "sns.set(context='paper', style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATTERN = '../scratch/full-runs-2020-05-26/run*/eval*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = glob.glob(os.path.expanduser(CSV_PATTERN))\n",
    "print(f\"Found {len(csv_paths)} CSV files\")\n",
    "loaded_csvs = [pd.read_csv(c) for c in csv_paths]\n",
    "frame = pd.concat(loaded_csvs)\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going to add some extra data:\n",
    "\n",
    "- Human-readable test variant names.\n",
    "- String-formatted mean and median for the LaTeX tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_preproc(env_name):\n",
    "    en = EnvName(env_name)\n",
    "    return en.name_prefix + en.demo_test_spec + en.version_suffix\n",
    "\n",
    "def to_variant(env_name):\n",
    "    en = EnvName(env_name)\n",
    "    return en.demo_test_spec.strip('-')\n",
    "\n",
    "def to_prefix(env_name):\n",
    "    en = EnvName(env_name)\n",
    "    return en.name_prefix.strip('-')\n",
    "\n",
    "# reformat env names\n",
    "frame['test_env'] = frame['test_env'].map(strip_preproc)\n",
    "frame['demo_env'] = frame['demo_env'].map(strip_preproc)\n",
    "frame['variant'] = frame['test_env'].map(to_variant)\n",
    "frame['env_prefix'] = frame['demo_env'].map(to_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist_names = [\n",
    "    \"bc-mt-ft\",\n",
    "    # \"bc-mt-aug\",\n",
    "    # \"bc-mt-allo\",\n",
    "    # \"gail-mt-aug\",\n",
    "    # \"gail-mt-allo\",\n",
    "    \"gail-trans\",\n",
    "]\n",
    "keep_mask = frame['run_id'].map(lambda name: not any(bl in name for bl in blacklist_names))\n",
    "print(f\"Stripping out {(~keep_mask).sum()}/{len(keep_mask)} entries\")\n",
    "frame = frame[keep_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate tables for each task\n",
    "\n",
    "Here I'm going to create separate LaTeX tables for each task. Each row will be a method, and each column will be a variant, with cells showing both mean and standard deviation of performance. Should have the same set of columns for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT_SHORT_NAMES = collections.OrderedDict([\n",
    "  (\"Demo\", \"Demo\"),\n",
    "  (\"TestJitter\", \"Jitter\"),\n",
    "  (\"TestLayout\", \"Layout\"),\n",
    "  (\"TestColour\", \"Colour\"),\n",
    "  (\"TestShape\", \"Shape\"),\n",
    "  (\"TestCountPlus\", \"CountPlus\"),\n",
    "  (\"TestDynamics\", \"Dynamics\"),\n",
    "  (\"TestAll\", \"All\"),\n",
    "])\n",
    "VARIANT_ORDER = {\n",
    "    env_name: index for index, env_name\n",
    "    in enumerate(VARIANT_SHORT_NAMES.keys())\n",
    "}\n",
    "PROBLEM_PREFIX_ORDER = [\n",
    "    'MoveToCorner',\n",
    "    'MoveToRegion',\n",
    "    'MatchRegions',\n",
    "    'MakeLine',\n",
    "    'FindDupe',\n",
    "    'FixColour',\n",
    "    'ClusterColour',\n",
    "    'ClusterShape',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to do some sanity checks on the variants to make sure that none are missing from `VARIANT_SHORT_NAMES` or `VARIANT_ORDER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variants = set(frame['variant'].unique())\n",
    "missing = all_variants - VARIANT_ORDER.keys()\n",
    "# if missing:\n",
    "#     print(f\"{len(missing)} variants are missing:\")\n",
    "#     print(sorted(missing))\n",
    "#     assert False, 'figure out what is going wrong!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighten(rgba):\n",
    "    \"\"\"Convert an RGBA colour to an RGB colour in which original alpha\n",
    "    channel is used to rescale luminance (hacky, but whatever).\"\"\"\n",
    "    # TODO: if you have time, design a custom colour space by just\n",
    "    # walking through a range of nice CIELAB colours (e.g. all the\n",
    "    # CIELAB blues).\n",
    "    assert len(rgba) == 4, len(rgba)\n",
    "    rgb = rgba[:3]\n",
    "    a = rgba = rgba[3]\n",
    "    h, l, s = colorsys.rgb_to_hls(*rgb)\n",
    "    l = 1 - (1 - l) * a\n",
    "    rgb = colorsys.hls_to_rgb(h, l, s)\n",
    "    return rgb\n",
    "\n",
    "def assign_colours(values, legend=False):\n",
    "    r\"\"\"Create the LaTeX \\cellcolor commands necessary for a series of values.\n",
    "    Values are assumed to be in [0,1].\"\"\"\n",
    "    # (requires \\usepackage[table]{xcolor}, per\n",
    "    # https://tex.stackexchange.com/a/50351)\n",
    "    values = np.asarray(values)\n",
    "    values[values < 0] = 0\n",
    "    values[values > 1] = 1\n",
    "    cmap = plt.get_cmap('Blues')\n",
    "    cols_rgba = cmap(values, alpha=0.2)\n",
    "    if isinstance(cols_rgba, tuple) and len(cols_rgba) == 4:\n",
    "        # annoying, when you give cmap() an array with a single\n",
    "        # element it returns a tuple instead of an ndarray\n",
    "        cols_rgba = np.asarray([cols_rgba])\n",
    "    cols_rgb = map(lighten, cols_rgba)\n",
    "    if legend:\n",
    "        # a crude LaTeX colour gradient\n",
    "        return r'\\!'.join(r'\\crule{%.2f,%.2f,%.2f}' % spec for spec in cols_rgb)\n",
    "    else:\n",
    "        specs = [\n",
    "            r'\\cellcolor[rgb]{%.2f,%.2f,%.2f}' % tuple(spec)\n",
    "            for spec in cols_rgb\n",
    "        ]\n",
    "    return specs\n",
    "\n",
    "\n",
    "def assign_colours_improved(values, legend=False):\n",
    "    # this is the colour for a value of 1.0; we lighten it in CIELab to get other values\n",
    "    base_colour_rgb = (180/255.0, 208/255.0, 248/255.0)\n",
    "    raise NotImplementedError(\n",
    "        \"doing this properly (with right colour space in LaTeX etc.) \"\n",
    "        \"seems like too much work, so skipping it for now\")\n",
    "\n",
    "def compute_cell_contents(tab):\n",
    "    (_, col_name), *_ = tab.columns.to_flat_index()\n",
    "    mean_series_means = tab['mean_score__reduced_mean'].squeeze(axis=1)\n",
    "    std_series_means = tab['std_score__reduced_mean'].squeeze(axis=1)\n",
    "    mean_series_stds = tab['mean_score__reduced_std'].squeeze(axis=1)\n",
    "    std_series_stds = tab['std_score__reduced_std'].squeeze(axis=1)\n",
    "    cols = assign_colours(mean_series_means)\n",
    "    result_list = [\n",
    "        # r'%s %.2f$\\pm$%.2f (%.2f$\\pm$%.2f)' % col_mu_std\n",
    "        r'{0} {1:.2f}$\\pm${2:.2f}'.format(*col_mu_std)\n",
    "        for col_mu_std in zip(cols, mean_series_means, mean_series_stds,\n",
    "                              std_series_means, std_series_stds)\n",
    "    ]\n",
    "    # now construct a new frame with column name taken from 'tab', and indices\n",
    "    # taken from 'mean_series' and 'std_series'\n",
    "    contents = pd.Series(data=result_list, index=tab.index, name=col_name)\n",
    "    return contents\n",
    "\n",
    "def flatten_top_n_levels(columns, n=2):\n",
    "    \"\"\"A bit like `MultiIndex.flatten_index()`, except it only flattens the\n",
    "    first `n` levels. Useful if you want to, e.g., flatten the top two levels\n",
    "    of a three-level multi-index (which I want to do during aggregation!)\"\"\"\n",
    "    flat_index = columns.to_flat_index()\n",
    "    new_index_list = []\n",
    "    for entry_tuple in flat_index:\n",
    "        new_tuple = ('__'.join(entry_tuple[:n]), ) + entry_tuple[n:]\n",
    "        new_index_list.append(new_tuple)\n",
    "    new_index = pd.MultiIndex.from_tuples(new_index_list)\n",
    "    return new_index\n",
    "\n",
    "def reduced_mean(vals):\n",
    "    # this function only exists to give a nice name to aggfunc in pivot table\n",
    "    return np.mean(vals)\n",
    "\n",
    "def reduced_std(vals):\n",
    "    # another function that only exists to give a nice name to aggfunc\n",
    "    return np.std(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_alg_name(alg_name):\n",
    "    global _fan_full_re  # cached regex\n",
    "    \n",
    "    if '_fan_full_re' not in globals():\n",
    "        # for parsing the environment at the end of the name\n",
    "        env_names = [\n",
    "            'move-to-corner', 'move-to-region', 'match-regions', 'make-line',\n",
    "             'find-dupe', 'fix-colour', 'cluster-shape', 'cluster-colour'\n",
    "        ]\n",
    "        env_re_s = f'(?P<env>{\"|\".join(env_names)})'\n",
    "        # for parsing the \"top-level\" algorithm that will be displayed in the table\n",
    "        top_levels = collections.OrderedDict([\n",
    "            # lack of closing parens is intentional! We'll insert them at the end,\n",
    "            # after adding ego/allo designator, augmentation info, etc.\n",
    "            ('bc-st-', 'BC (ST'),\n",
    "            ('bc-mt-', 'BC (MT'),\n",
    "            ('bc-mt-ft-', 'BC (FT-MT'),\n",
    "            ('gail-st-', 'GAIL (ST'),\n",
    "            ('gail-mt-', 'GAIL (MT'),\n",
    "            # this is the catch-all\n",
    "            ('gail-', 'GAIL (ST'),\n",
    "        ])\n",
    "        top_level_re_s = f'(?P<top_level>{\"|\".join(top_levels)})'\n",
    "        # for parsing any augmentation ablations\n",
    "        augmentations = {\n",
    "            'aug-no-col-': 'no col. aug.',\n",
    "            'aug-no-trans-': 'no trans./rot. aug.',\n",
    "            'aug-none-': 'no aug.',\n",
    "        }\n",
    "        aug_re_s = f'(?P<aug>{\"|\".join(augmentations)})'\n",
    "        preprocs = {'allo-': 'allo.'}\n",
    "        preproc_re_s = f'(?P<preproc>{\"|\".join(preprocs)})'\n",
    "        variants = {\n",
    "            'demo': 'trans. to Demo',\n",
    "            'jitter': 'trans. to Jitter',\n",
    "            'layout': 'trans. to Layout',\n",
    "            'colour': 'trans. to Colour',\n",
    "            'shape': 'trans. to Shape',\n",
    "            'countplus': 'trans. to CountPlus',\n",
    "            'dynamics': 'trans. to Dynamics',\n",
    "            'all': 'trans. to All',\n",
    "        }\n",
    "        trans_re_s = f'(trans-(?P<trans_env>{\"|\".join(variants)})-)'\n",
    "        full_re_s = f'{top_level_re_s}{preproc_re_s}?{aug_re_s}?{trans_re_s}?on-{env_re_s}'\n",
    "        full_re = re.compile(full_re_s)\n",
    "    else:\n",
    "        full_re = _fan_full_re\n",
    "\n",
    "    match = full_re.match(alg_name)\n",
    "    if not match:\n",
    "        # couldn't process :(\n",
    "        return alg_name\n",
    "\n",
    "    # can also access match.group('env') if needed\n",
    "    match_top_level = top_levels.get(match.group('top_level'))\n",
    "    match_aug = augmentations.get(match.group('aug'))\n",
    "    match_trans = variants.get(match.group('trans_env'))\n",
    "    match_preproc = preprocs.get(match.group('preproc'))\n",
    "    # the top level includes an opening paren, but not a closing paren\n",
    "    result_parts = [match_top_level]\n",
    "    designators = (match_preproc, match_aug, match_trans)\n",
    "    if any(designators):\n",
    "        result_parts.append(', ')\n",
    "        result_parts.append(', '.join(filter(bool, designators)))\n",
    "    result_parts.append(')')\n",
    "    # TODO: when you actually go to use this, double-check that it doesn't\n",
    "    # accidentally alias unrelated things\n",
    "    return ''.join(result_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code to produce a full table of **all** results. This is pretty dense; in the next cell, I'm going to summarise it by averaging results for each variant over all tasks (this is only meaningful for some variants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Colour scale:', assign_colours(np.linspace(0, 1, 8), legend=True))\n",
    "print('')\n",
    "\n",
    "demo_envs = sorted(\n",
    "    frame['demo_env'].unique(),\n",
    "    key=lambda s: PROBLEM_PREFIX_ORDER.index(s.split('-')[0]))\n",
    "tables_as_latex = []\n",
    "for demo_env in demo_envs:\n",
    "    prefix = to_prefix(demo_env)\n",
    "    subset = frame[frame['demo_env'] == demo_env]\n",
    "    pivot = pd.pivot_table(\n",
    "        subset, index='latex_alg_name', columns='variant',\n",
    "        values=('mean_score', 'std_score'),\n",
    "        aggfunc=(reduced_mean, reduced_std),\n",
    "        dropna=False)\n",
    "    pivot.columns = flatten_top_n_levels(pivot.columns, 2)\n",
    "    renamed_pivot = pivot.groupby(axis=1, level=1).apply(compute_cell_contents)\n",
    "    for col_name in VARIANT_SHORT_NAMES:\n",
    "        if col_name not in renamed_pivot.columns:\n",
    "            renamed_pivot[col_name] = '-'\n",
    "    sorted_pivot = renamed_pivot[list(VARIANT_SHORT_NAMES)]\n",
    "    sorted_cols = [VARIANT_SHORT_NAMES.get(c, c) for c in sorted_pivot.columns]\n",
    "    sorted_pivot.columns = pd.MultiIndex.from_product([(r'\\textbf{%s}' % prefix, ), sorted_cols], names=('Task', 'Variant'))\n",
    "    sorted_pivot.index = sorted_pivot.index.map(format_alg_name).rename(\"Method\")\n",
    "    sorted_pivot.sort_index(axis=0, inplace=True)\n",
    "    with pd.option_context(\"max_colwidth\", 1000):\n",
    "        # max_colwidth is dealing with weird pandas bug:\n",
    "        # https://github.com/pandas-dev/pandas/issues/6491\n",
    "        latex_formatted = sorted_pivot.to_latex(\n",
    "            # label=f'tab:res-{prefix.lower()}',\n",
    "            column_format='l' + 'c' * len(sorted_cols),\n",
    "            bold_rows=False,\n",
    "            escape=False,\n",
    "        ).replace('{l}', '{c}')\n",
    "\n",
    "    latex_lines = []\n",
    "    for line in latex_formatted.splitlines():\n",
    "        # throw out useless \"method\" line\n",
    "        if line.startswith('Method  '):\n",
    "            continue\n",
    "        to_strip = ('Task ', 'Variant ')\n",
    "        for t in to_strip:\n",
    "            if line.startswith(t):\n",
    "                line = ' ' * len(t) + line[len(t):]\n",
    "        latex_lines.append(line)\n",
    "    tables_as_latex.append(latex_lines)\n",
    "    \n",
    "tabs_per_out = 2\n",
    "for tab_start in range(0, len(tables_as_latex), tabs_per_out):\n",
    "    sub_tabs = tables_as_latex[tab_start:tab_start + tabs_per_out]\n",
    "    out_lines = []\n",
    "    for tab_num, latex_lines in enumerate(sub_tabs, start=1):\n",
    "        if not out_lines:\n",
    "            # keep first few lines (including the \\begin{tabular})\n",
    "            out_lines.extend(latex_lines[:2])\n",
    "        out_lines.extend(latex_lines[2:-2])\n",
    "        if tab_num == len(sub_tabs):\n",
    "            out_lines.extend(latex_lines[-2:])\n",
    "        else:\n",
    "            out_lines.extend([r'\\midrule'])\n",
    "    print('\\n'.join(out_lines))\n",
    "    print('\\n' * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is a version of the tables above where we just summarise over each variant. In the colour/shape case, this is a little misleading, since some tasks benefit from randomisation of those attributes, while others do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_frame = frame.copy()\n",
    "variant_frame['latex_alg_name'] = variant_frame['latex_alg_name'].map(format_alg_name)\n",
    "variant_pivot = pd.pivot_table(\n",
    "    variant_frame, index='latex_alg_name', columns='variant',\n",
    "    values=('mean_score', 'std_score'),\n",
    "    aggfunc=(reduced_mean, reduced_std),\n",
    "    dropna=False)\n",
    "variant_pivot.columns = flatten_top_n_levels(variant_pivot.columns, 2)\n",
    "renamed_variant_pivot = variant_pivot.groupby(axis=1, level=1).apply(compute_cell_contents)\n",
    "renamed_variant_pivot = renamed_variant_pivot[list(VARIANT_SHORT_NAMES)]\n",
    "renamed_variant_pivot.index = renamed_variant_pivot.index.rename(\"Method\")\n",
    "variant_sorted_cols = [r'\\textbf{' + VARIANT_SHORT_NAMES.get(c, c) + '}' for c in renamed_variant_pivot.columns]\n",
    "renamed_variant_pivot.columns = pd.CategoricalIndex(variant_sorted_cols)\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    # max_colwidth is dealing with weird pandas bug:\n",
    "    # https://github.com/pandas-dev/pandas/issues/6491\n",
    "    variant_latex_formatted = renamed_variant_pivot.to_latex(\n",
    "        # label=f'tab:res-{prefix.lower()}',\n",
    "        column_format='l' + 'c' * len(variant_sorted_cols),\n",
    "        bold_rows=True,\n",
    "        escape=False,\n",
    "    ).replace('{l}', '{c}')\n",
    "print(variant_latex_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered version of the above for final paper\n",
    "variant_frame_reduced = frame.copy()\n",
    "variant_frame_reduced['latex_alg_name'] = variant_frame_reduced['latex_alg_name'].map(format_alg_name)\n",
    "variant_pivot_reduced = pd.pivot_table(\n",
    "    variant_frame_reduced, index='latex_alg_name', columns='variant',\n",
    "    values=('mean_score', 'std_score'),\n",
    "    aggfunc=(reduced_mean, reduced_std),\n",
    "    dropna=False)\n",
    "variant_pivot_reduced.columns = flatten_top_n_levels(variant_pivot_reduced.columns, 2)\n",
    "renamed_variant_pivot_reduced = variant_pivot_reduced.groupby(axis=1, level=1).apply(compute_cell_contents)\n",
    "variant_shorter_names = collections.OrderedDict([\n",
    "  (\"Demo\", \"Demo\"),\n",
    "  (\"TestJitter\", \"Jitter\"),\n",
    "  (\"TestLayout\", \"Layout\"),\n",
    "  (\"TestColour\", \"Colour\"),\n",
    "  (\"TestShape\", \"Shape\"),\n",
    "  # (\"TestCountPlus\", \"CountPlus\"),\n",
    "  # (\"TestDynamics\", \"Dynamics\"),\n",
    "  # (\"TestAll\", \"All\"),\n",
    "])\n",
    "renamed_variant_pivot_reduced = renamed_variant_pivot_reduced[list(variant_shorter_names)]\n",
    "renamed_variant_pivot_reduced.index = renamed_variant_pivot_reduced.index.rename(\"Method\")\n",
    "variant_sorted_cols_reduced = [VARIANT_SHORT_NAMES.get(c, c) for c in renamed_variant_pivot_reduced.columns]\n",
    "renamed_variant_pivot_reduced.columns = pd.CategoricalIndex(variant_sorted_cols_reduced)\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    # max_colwidth is dealing with weird pandas bug:\n",
    "    # https://github.com/pandas-dev/pandas/issues/6491\n",
    "    variant_latex_formatted_reduced = renamed_variant_pivot_reduced.to_latex(\n",
    "        # label=f'tab:res-{prefix.lower()}',\n",
    "        column_format='l' + 'c' * len(variant_sorted_cols_reduced),\n",
    "        bold_rows=True,\n",
    "        escape=False,\n",
    "    ).replace('{l}', '{c}')\n",
    "print(variant_latex_formatted_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same thing again, but with Markdown for the README\n",
    "\n",
    "def compute_cell_contents(tab):\n",
    "    (_, col_name), *_ = tab.columns.to_flat_index()\n",
    "    mean_series_means = tab['mean_score__reduced_mean'].squeeze(axis=1)\n",
    "    std_series_means = tab['std_score__reduced_mean'].squeeze(axis=1)\n",
    "    mean_series_stds = tab['mean_score__reduced_std'].squeeze(axis=1)\n",
    "    std_series_stds = tab['std_score__reduced_std'].squeeze(axis=1)\n",
    "    cols = assign_colours(mean_series_means)\n",
    "    result_list = [\n",
    "        r'{1:.2f}±{2:.2f}'.format(*col_mu_std)\n",
    "        for col_mu_std in zip(cols, mean_series_means, mean_series_stds,\n",
    "                              std_series_means, std_series_stds)\n",
    "    ]\n",
    "    # now construct a new frame with column name taken from 'tab', and indices\n",
    "    # taken from 'mean_series' and 'std_series'\n",
    "    contents = pd.Series(data=result_list, index=tab.index, name=col_name)\n",
    "    return contents\n",
    "\n",
    "variant_frame_reduced = frame.copy()\n",
    "variant_frame_reduced['latex_alg_name'] = variant_frame_reduced['latex_alg_name'].map(format_alg_name)\n",
    "variant_pivot_reduced = pd.pivot_table(\n",
    "    variant_frame_reduced, index='latex_alg_name', columns='variant',\n",
    "    values=('mean_score', 'std_score'),\n",
    "    aggfunc=(reduced_mean, reduced_std),\n",
    "    dropna=False)\n",
    "variant_pivot_reduced.columns = flatten_top_n_levels(variant_pivot_reduced.columns, 2)\n",
    "renamed_variant_pivot_reduced = variant_pivot_reduced.groupby(axis=1, level=1).apply(compute_cell_contents)\n",
    "renamed_variant_pivot_reduced = renamed_variant_pivot_reduced[list(variant_shorter_names)]\n",
    "renamed_variant_pivot_reduced.index = renamed_variant_pivot_reduced.index.rename(\"Method\")\n",
    "variant_sorted_cols_reduced = [VARIANT_SHORT_NAMES.get(c, c) for c in renamed_variant_pivot_reduced.columns]\n",
    "renamed_variant_pivot_reduced.columns = pd.CategoricalIndex(variant_sorted_cols_reduced)\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    variant_markdown_formatted_reduced = renamed_variant_pivot_reduced.to_markdown()\n",
    "print(variant_markdown_formatted_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out average running time\n",
    "\n",
    "GAIL is easy because the logger records cumulative time by default. BC is much harder; I need to kind of triangulate it from debug.log :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROGRESS_CSV_PATTERN = '../scratch/full-runs-2020-05-26/run_gail*/progress.csv'\n",
    "progress_paths = glob.glob(os.path.expanduser(PROGRESS_CSV_PATTERN))\n",
    "print(f\"Found {len(progress_paths)} progress.csv files\")\n",
    "times = []\n",
    "for progress_path in progress_paths:\n",
    "    time_csv = pd.read_csv(progress_path)\n",
    "    try:\n",
    "        times.append(time_csv['CumTime (s)'].max())\n",
    "    except KeyError as ex:\n",
    "        raise KeyError(f\"exception when processing '{progress_path}': {ex}\")\n",
    "print(f'Mean time (h): {np.mean(times) / (60*60)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser\n",
    "import datetime\n",
    "\n",
    "DEBUG_LOG_PATTERN = '../scratch/full-runs-2020-05-26/run_bc*/debug.log'\n",
    "debug_log_paths = glob.glob(os.path.expanduser(DEBUG_LOG_PATTERN))\n",
    "print(f\"Found {len(debug_log_paths)} progress.csv files\")\n",
    "bc_durations = []\n",
    "hour = datetime.timedelta(hours=1)\n",
    "for debug_log_path in debug_log_paths:\n",
    "    with open(debug_log_path, 'r') as fp:\n",
    "        first_line = None\n",
    "        last_line = None\n",
    "        for l in fp:\n",
    "            l = l.strip()\n",
    "            if not l:\n",
    "                continue\n",
    "            last_line, *_ = l.split('|', 1)\n",
    "            if first_line is None:\n",
    "                first_line = last_line\n",
    "        if last_line is not None:\n",
    "            first_time = dateparser.parse(first_line)\n",
    "            last_time = dateparser.parse(last_line)\n",
    "            bc_durations.append((last_time - first_time) / hour)\n",
    "        else:\n",
    "            print(f\"Couldn't parse times from {debug_log_path} (??)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean running time: {np.mean(bc_durations)}h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
